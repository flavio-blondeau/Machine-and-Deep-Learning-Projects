{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1AGDzcfIN_O4a0jkJ2vPHxpO0FSW38MRl",
      "authorship_tag": "ABX9TyNB3gP/WQTwj4hMVshgHDr7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "PpkwdtOcDshZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.special import logsumexp\n",
        "from typing import List, Tuple"
      ],
      "metadata": {
        "id": "xsOB-o0e95BS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XULv_65qhmkW",
        "outputId": "038dbe48-5882-457b-d3d4-904367440a82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at mnt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"mnt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"mnt/My Drive/Colab Notebooks\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVrlqbgOiYai",
        "outputId": "ce535eae-a99d-49ed-ae3f-d783b8eab06d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mnt/My Drive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install import-ipynb\n",
        "import import_ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yED6UZhgicKv",
        "outputId": "89fa4e6d-f732-4f4c-ad7c-ede7b85ca14a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting import-ipynb\n",
            "  Downloading import_ipynb-0.1.4-py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.8/dist-packages (from import-ipynb) (7.9.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.8/dist-packages (from import-ipynb) (5.7.3)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from IPython->import-ipynb) (5.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from IPython->import-ipynb) (57.4.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from IPython->import-ipynb) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from IPython->import-ipynb) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from IPython->import-ipynb) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from IPython->import-ipynb) (4.4.2)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from IPython->import-ipynb) (2.0.10)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from IPython->import-ipynb) (0.2.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.8/dist-packages (from nbformat->import-ipynb) (2.16.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.8/dist-packages (from nbformat->import-ipynb) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.8/dist-packages (from nbformat->import-ipynb) (5.2.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->IPython->import-ipynb) (0.8.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (22.2.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (5.10.2)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.19.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython->import-ipynb) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython->import-ipynb) (0.2.6)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.8/dist-packages (from jupyter-core->nbformat->import-ipynb) (3.0.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect->IPython->import-ipynb) (0.7.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->import-ipynb) (3.12.1)\n",
            "Installing collected packages: jedi, import-ipynb\n",
            "Successfully installed import-ipynb-0.1.4 jedi-0.18.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports from Neural_Network_from_scratch notebook\n",
        "from Neural_Network_from_scratch import Loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xGGNCjuigbO",
        "outputId": "1a522702-5de0-4f77-ea6a-d8cdad2df8eb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "importing Jupyter notebook from Neural_Network_from_scratch.ipynb\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss after 10 epochs is 37.226.\n",
            "Validation loss after 20 epochs is 29.312.\n",
            "Validation loss after 30 epochs is 26.731.\n",
            "Validation loss after 40 epochs is 26.571.\n",
            "Loss increased after epoch 50, the final loss was 26.571, using the model from epoch 40\n",
            "\n",
            "Mean absolute error: 3.68\n",
            "\n",
            "Root mean squared error 5.20\n",
            "Validation loss after 10 epochs is 35.272.\n",
            "Validation loss after 20 epochs is 20.043.\n",
            "Validation loss after 30 epochs is 17.688.\n",
            "Validation loss after 40 epochs is 15.706.\n",
            "Validation loss after 50 epochs is 14.666.\n",
            "\n",
            "Mean absolute error: 2.53\n",
            "\n",
            "Root mean squared error 3.83\n",
            "Validation loss after 10 epochs is 89.403.\n",
            "Validation loss after 20 epochs is 19.193.\n",
            "Validation loss after 30 epochs is 16.681.\n",
            "Validation loss after 40 epochs is 14.424.\n",
            "Validation loss after 50 epochs is 12.989.\n",
            "\n",
            "Mean absolute error: 2.37\n",
            "\n",
            "Root mean squared error 3.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Softmax Loss"
      ],
      "metadata": {
        "id": "giWwl-1lDh-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subclass for Softmax Crossentropy Loss\n",
        "\n",
        "def softmax(x, axis=None): # Softmax function\n",
        "  return np.exp(x - logsumexp(x, axis=axis, keepdims=True))\n",
        "\n",
        "def normalize(a: np.ndarray):\n",
        "  other = 1 - a\n",
        "  return np.concatenate([a, other], axis=1)\n",
        "\n",
        "def unnormalize(a: np.ndarray):\n",
        "  return a[np.newaxis, 0]\n",
        "\n",
        "\n",
        "class Softmax(Loss):\n",
        "  def __init__(self, eps: float = 1e-9) -> None:\n",
        "    super().__init__()\n",
        "    self.eps = eps # Store epsilon\n",
        "    self.single_output = False # Store single output NN\n",
        "\n",
        "  \n",
        "  def _output(self) -> float:\n",
        "    # Check if the NN has only a single output (i.e. only one class)\n",
        "    if self.target.shape[1] == 0:\n",
        "      self.single_output = True\n",
        "\n",
        "    # If there is a single output, normalize it\n",
        "    if self.single_output:\n",
        "      self.prediction = normalize(self.prediction)\n",
        "      self.target = normalize(self.target)\n",
        "\n",
        "    softmax_preds = softmax(self.prediction, axis=1) # Apply softmax function to each row\n",
        "\n",
        "    self.softmax_preds = np.clip(softmax_preds, self.eps, 1 - self.eps) # Clip softmax output to prevent numeric instability\n",
        "\n",
        "    softmax_loss = (-1.0 * self.target * np.log(self.softmax_preds) - (1.0 - self.target) * np.log(1 - self.softmax_preds)) # Loss computation\n",
        "\n",
        "    return np.sum(softmax_loss) / self.prediction.shape[0]\n",
        "\n",
        "\n",
        "  def _input_grad(self) -> np.ndarray:\n",
        "    if self.single_output:\n",
        "      return unnormalize(self.softmax_preds - self.target)\n",
        "    else:\n",
        "      return (self.softmax_preds - self.target) / self.prediction.shape[0]"
      ],
      "metadata": {
        "id": "FB3ui_dxiyR4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST dataset"
      ],
      "metadata": {
        "id": "aA6K5vB_Dwfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import MNIST dataset - Credit: https://github.com/hsjeong5\n",
        "\n",
        "from urllib import request\n",
        "import gzip\n",
        "import pickle\n",
        "\n",
        "filename = [\n",
        "[\"training_images\",\"train-images-idx3-ubyte.gz\"],\n",
        "[\"test_images\",\"t10k-images-idx3-ubyte.gz\"],\n",
        "[\"training_labels\",\"train-labels-idx1-ubyte.gz\"],\n",
        "[\"test_labels\",\"t10k-labels-idx1-ubyte.gz\"]\n",
        "]\n",
        "\n",
        "\n",
        "def download_mnist():\n",
        "  base_url = \"http://yann.lecun.com/exdb/mnist/\"\n",
        "  for name in filename:\n",
        "    print(\"Downloading \"+name[1]+\"...\")\n",
        "    request.urlretrieve(base_url+name[1], name[1])\n",
        "  print(\"Download complete.\")\n",
        "\n",
        "\n",
        "def save_mnist():\n",
        "  mnist = {}\n",
        "  for name in filename[:2]:\n",
        "    with gzip.open(name[1], 'rb') as f:\n",
        "      mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28)\n",
        "  for name in filename[-2:]:\n",
        "    with gzip.open(name[1], 'rb') as f:\n",
        "      mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "  with open(\"mnist.pkl\", 'wb') as f:\n",
        "    pickle.dump(mnist,f)\n",
        "  print(\"Save complete.\")\n",
        "\n",
        "\n",
        "def init():\n",
        "  download_mnist()\n",
        "  save_mnist()\n",
        "\n",
        "\n",
        "def load():\n",
        "  with open(\"mnist.pkl\",'rb') as f:\n",
        "    mnist = pickle.load(f)\n",
        "  return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCfxA7AADwMA",
        "outputId": "452c9d94-ed73-4b09-ecc8-18a738d0efd5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading train-images-idx3-ubyte.gz...\n",
            "Downloading t10k-images-idx3-ubyte.gz...\n",
            "Downloading train-labels-idx1-ubyte.gz...\n",
            "Downloading t10k-labels-idx1-ubyte.gz...\n",
            "Download complete.\n",
            "Save complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_test, y_test = load()\n",
        "num_labels = len(y_train)\n",
        "num_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmnHb1lkENJn",
        "outputId": "94058dd8-ab05-4224-d299-4c0a849c6881"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encoding labels (both train and test)\n",
        "num_labels = len(y_train)\n",
        "train_labels = np.zeros((num_labels, 10))\n",
        "for i in range(num_labels):\n",
        "  train_labels[i][y_train[i]] = 1\n",
        "\n",
        "num_labels = len(y_test)\n",
        "test_labels = np.zeros((num_labels, 10))\n",
        "for i in range(num_labels):\n",
        "  test_labels[i][y_test[i]] = 1"
      ],
      "metadata": {
        "id": "neRQS0dTFVI0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize data (mean 0, variance 1)\n",
        "X_train, X_test = X_train - np.mean(X_train), X_test - np.mean(X_train)\n",
        "X_train, X_test = X_train / np.std(X_train), X_test / np.std(X_train)"
      ],
      "metadata": {
        "id": "EZTnxAeIGCiz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for measuring accuracy\n",
        "def model_accuracy(model, test_set):\n",
        "  return print(f\"The model validation accuracy is: {np.equal(np.argmax(model.forward(test_set), axis=1), y_test).sum() * 100.0 / test_set.shape[0]:.2f}%\")\n"
      ],
      "metadata": {
        "id": "CNxJkByWGdvZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports from NN_from_scratch\n",
        "from Neural_Network_from_scratch import NeuralNetwork, Dense, MeanSquaredError, SGD, Sigmoid, Tanh, Linear, ReLU, Trainer"
      ],
      "metadata": {
        "id": "_cKjlK3bG01I"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with sigmoid activation\n",
        "model = NeuralNetwork(\n",
        "    layers = [Dense(neurons = 89, activation = Tanh()),\n",
        "              Dense(neurons = 10, activation = Sigmoid())],\n",
        "    loss = MeanSquaredError()\n",
        ")\n",
        "\n",
        "trainer = Trainer(model, SGD(learning_rate = 0.1))\n",
        "\n",
        "trainer.fit(X_train, train_labels, X_test, test_labels, epochs = 50, eval_every = 10, batch_size = 60)\n",
        "print()\n",
        "model_accuracy(model, X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PnEDX_cHO1X",
        "outputId": "bd669cc0-8a88-482a-8ed6-1d780a2631e0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss after 10 epochs is 0.622.\n",
            "Validation loss after 20 epochs is 0.579.\n",
            "Validation loss after 30 epochs is 0.474.\n",
            "Validation loss after 40 epochs is 0.435.\n",
            "Validation loss after 50 epochs is 0.376.\n",
            "\n",
            "The model validation accuracy is: 72.19%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model with sigmoid activation and softmax\n",
        "model = NeuralNetwork(\n",
        "    layers = [Dense(neurons = 89, activation = Sigmoid()),\n",
        "              Dense(neurons = 10, activation = Linear())],\n",
        "    loss = Softmax()\n",
        ")\n",
        "\n",
        "trainer = Trainer(model, SGD(learning_rate = 0.1))\n",
        "\n",
        "trainer.fit(X_train, train_labels, X_test, test_labels, epochs = 120, eval_every = 5, batch_size = 60)\n",
        "print()\n",
        "model_accuracy(model, X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSd7eMLZK44F",
        "outputId": "a483ef6f-431a-4d30-dfb8-84d225a961f2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss after 5 epochs is 0.716.\n",
            "Validation loss after 10 epochs is 0.603.\n",
            "Validation loss after 15 epochs is 0.551.\n",
            "Validation loss after 20 epochs is 0.523.\n",
            "Validation loss after 25 epochs is 0.504.\n",
            "Validation loss after 30 epochs is 0.491.\n",
            "Validation loss after 35 epochs is 0.479.\n",
            "Validation loss after 40 epochs is 0.472.\n",
            "Validation loss after 45 epochs is 0.468.\n",
            "Validation loss after 50 epochs is 0.465.\n",
            "Validation loss after 55 epochs is 0.461.\n",
            "Validation loss after 60 epochs is 0.460.\n",
            "Validation loss after 65 epochs is 0.456.\n",
            "Validation loss after 70 epochs is 0.455.\n",
            "Validation loss after 75 epochs is 0.454.\n",
            "Loss increased after epoch 80, the final loss was 0.454, using the model from epoch 75\n",
            "\n",
            "The model validation accuracy is: 92.37%\n"
          ]
        }
      ]
    }
  ]
}
