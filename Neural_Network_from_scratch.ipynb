{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNB+niWKGlIXbGRKGo4mFs4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part One: Deep Learning Architecture"
      ],
      "metadata": {
        "id": "zdFVTXT34EYE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iCKDkVIlOQTW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that check dimension compatibility between ndarrays\n",
        "def assert_same_shape(array1: np.ndarray, array2: np.ndarray):\n",
        "    assert array1.shape == array2.shape, 'The two arrays have incompatible shapes: {} and {}'.format(tuple(array1.shape), tuple(array2.shape))\n",
        "    return None"
      ],
      "metadata": {
        "id": "mO9YTRtz9FHQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Operation classes"
      ],
      "metadata": {
        "id": "nl074_LqIbq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We first make a class for a basic operation, which comprehends both matrix operations (as matmul) and element-wise ones (as sigmoid)\n",
        "class Operation(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  def _output(self) -> np.ndarray: # This method must be defined for each Operation\n",
        "    raise NotImplementedError()\n",
        "\n",
        "\n",
        "  def forward(self, input_: np.ndarray):\n",
        "    self.input_ = input_ # Store input_ in the self.input_ instance variable\n",
        "    self.output = self._output() # Call the self._output() function\n",
        "\n",
        "    return self.output\n",
        "\n",
        "\n",
        "  def _input_grad(self, output_grad: np.ndarray) -> np.ndarray: # This method must be defined for each Operation\n",
        "    raise NotImplementedError()\n",
        "\n",
        "\n",
        "  def backward(self, output_grad: np.ndarray) -> np.ndarray:\n",
        "    assert_same_shape(self.output, output_grad) # Check that shapes match\n",
        "\n",
        "    self.input_grad = self._input_grad(output_grad) # Call the self._input_grad() function\n",
        "\n",
        "    assert_same_shape(self.input_, self.input_grad) # Check that shapes match\n",
        "\n",
        "    return self.input_grad"
      ],
      "metadata": {
        "id": "21rMXRae2pT9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We also define a subclass for operations involving parameters (weights and biases)\n",
        "class ParametersOperation(Operation):\n",
        "\n",
        "  def __init__(self, param: np.ndarray) -> np.ndarray: # ParameterOperation Method\n",
        "    super().__init__()\n",
        "    self.param = param\n",
        "\n",
        "\n",
        "  def _param_grad(self, output_grad: np.ndarray) -> np.ndarray: # This method must be defined for each ParameterOperation\n",
        "    raise NotImplementedError()\n",
        "\n",
        "\n",
        "  def backward(self, output_grad: np.ndarray) -> np.ndarray:\n",
        "    assert_same_shape(self.output, output_grad) # Check that shapes match\n",
        "\n",
        "    self.input_grad = self._input_grad(output_grad) # Call the self._input_grad() function\n",
        "    self.param_grad = self._param_grad(output_grad) # Call the self._param_grad() function\n",
        "\n",
        "    assert_same_shape(self.input_, self.input_grad) # Check that shapes match\n",
        "    assert_same_shape(self.param, self.param_grad)\n",
        "\n",
        "    return self.input_grad "
      ],
      "metadata": {
        "id": "ZhUdpXfgIjn7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Subclass for matrix multiplication between wieghts and input\n",
        "class WeightMul(ParametersOperation):\n",
        "\n",
        "  def __init__(self, W: np.ndarray):\n",
        "    super().__init__(W) # Initialize operation with self.param = W\n",
        "\n",
        "\n",
        "  def _output(self) -> np.ndarray: # Compute matrix multiplication\n",
        "    return np.dot(self.input_, self.param) \n",
        "\n",
        "\n",
        "  def _input_grad(self, output_grad: np.ndarray) -> np.ndarray: # Compute the gradient of the input\n",
        "    return np.dot(output_grad, np.transpose(self.param, (1,0)))\n",
        "\n",
        "\n",
        "  def _param_grad(self, output_grad: np.ndarray) -> np.ndarray: # Compute the gradient of the parameter\n",
        "    return np.dot(np.transpose(self.input_, (1,0)), output_grad)"
      ],
      "metadata": {
        "id": "g5trW3YNLVnl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Subclass for adding bias\n",
        "class BiasAdd(ParametersOperation):\n",
        "\n",
        "  def __init__(self, b: np.ndarray):\n",
        "    assert b.shape[0] == 1 # Check if b has the appropriate shape\n",
        "\n",
        "    super().__init__(b) # Initialize operation with self.param = b\n",
        "\n",
        "\n",
        "  def _output(self) -> np.ndarray: # Compute sum with bias\n",
        "    return self.input_ + self.param\n",
        "\n",
        "  \n",
        "  def _input_grad(self, output_grad: np.ndarray) -> np.ndarray: # Compute the gradient of the input\n",
        "    return np.ones_like(self.input_) * output_grad\n",
        "\n",
        "\n",
        "  def _param_grad(self, output_grad: np.ndarray) -> np.ndarray: # Compute the gradient of the parameter\n",
        "      output_grad_reshaped = np.sum(output_grad, axis=0).reshape(1, -1)\n",
        "      param_grad = np.ones_like(self.param)\n",
        "      return param_grad * output_grad_reshaped"
      ],
      "metadata": {
        "id": "-j9j6w0UPp0s"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activation functions"
      ],
      "metadata": {
        "id": "fUX60BIuxfSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subclass for the sigmoid activation function\n",
        "class Sigmoid(Operation):\n",
        "  def __init__(self) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "\n",
        "  def _output(self) -> np.ndarray: # Apply sigmoid element-wise\n",
        "    return 1.0 / (1.0 + np.exp(-1.0 * self.input_))\n",
        "\n",
        "\n",
        "  def _input_grad(self, output_grad: np.ndarray) -> np.ndarray: # Compute the gradient of the input\n",
        "    sigmoid_backward = self.output * (1.0 - self.output)\n",
        "    input_grad = sigmoid_backward * output_grad\n",
        "    return input_grad"
      ],
      "metadata": {
        "id": "G6u9MIRXRqpD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Subclass for the Linear activation function (it does nothing)\n",
        "class Linear(Operation):\n",
        "    def __init__(self) -> None:        \n",
        "        super().__init__()\n",
        "\n",
        "    def _output(self) -> np.ndarray:\n",
        "        return self.input_\n",
        "\n",
        "    def _input_grad(self, output_grad: np.ndarray) -> np.ndarray:\n",
        "        return output_grad"
      ],
      "metadata": {
        "id": "b3er0hKo5Xc2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Subclass for the Rectified Linear Unit activation function\n",
        "class ReLU(Operation):\n",
        "  def __init__(self) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "\n",
        "  def _output(self) -> np.ndarray: # Apply ReLU element-wise\n",
        "    return np.clip(self.input_, 0, None)\n",
        "\n",
        "\n",
        "  def _input_grad(self, output_grad: np.ndarray) -> np.ndarray: # Compute the gradient of the input\n",
        "    non_neg = self.output >= 0\n",
        "    return output_grad * non_neg"
      ],
      "metadata": {
        "id": "IJ8zSnDExjaD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Subclass for the Hyperbolic Tangent activation function\n",
        "class Tanh(Operation):\n",
        "  def __init__(self) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "\n",
        "  def _output(self) -> np.ndarray: # Apply tanh element-wise\n",
        "    return np.tanh(self.input_)\n",
        "\n",
        "\n",
        "  def _input_grad(self, output_grad: np.ndarray) -> np.ndarray: # Compute the gradient of the input\n",
        "    return output_grad * (1 - self.output * self.output)"
      ],
      "metadata": {
        "id": "O7a8BEdOyXUE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer and Dense classes"
      ],
      "metadata": {
        "id": "tx3S0AtzlA9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We define a class for a single layer of our Neural Network\n",
        "class Layer(object):\n",
        "  def __init__(self, neurons: int): \n",
        "    self.neurons = neurons # The number of units in the layer\n",
        "    self.first = True # Check if the layer is the first one\n",
        "    self.params: List[np.ndarray] = [] # List of parameters\n",
        "    self.param_grads: List[np.ndarray] = [] # List of parameter gradients\n",
        "    self.operations: List[Operation] = [] # List of operations\n",
        "\n",
        "\n",
        "  def _setup_layer(self, num_in: int) -> None: # This method must be defined for each Layer\n",
        "    raise NotImplementedError()\n",
        "\n",
        "\n",
        "  def _params(self) -> np.ndarray:\n",
        "    self.params = [] # Empty params\n",
        "\n",
        "    # Extract the _params from layer's operations with parameters\n",
        "    for operation in self.operations:\n",
        "      if issubclass(operation.__class__, ParametersOperation):\n",
        "        self.params.append(operation.param)\n",
        "\n",
        "\n",
        "  def forward(self, input_: np.ndarray) -> np.ndarray:\n",
        "    # Set up the first layer\n",
        "    if self.first:\n",
        "      self._setup_layer(input_)\n",
        "      self.first = False\n",
        "\n",
        "    self.input_ = input_ # Store input_ variable\n",
        "\n",
        "    # Perform the operations\n",
        "    for operation in self.operations:\n",
        "      input_ = operation.forward(input_)\n",
        "\n",
        "    self.output = input_ # Store input_ variable after the operations\n",
        "\n",
        "    return self.output\n",
        "\n",
        "\n",
        "  def _param_grads(self) -> np.ndarray:\n",
        "    self.param_grads = [] # Empty param_grads\n",
        "\n",
        "    # Extract the _param_grads from layer's operations with parameters\n",
        "    for operation in self.operations:\n",
        "      if issubclass(operation.__class__, ParametersOperation):\n",
        "        self.param_grads.append(operation.param_grad)\n",
        "\n",
        "\n",
        "  def backward(self, output_grad: np.ndarray) -> np.ndarray:\n",
        "    assert_same_shape(self.output, output_grad) # Check that shapes match\n",
        "\n",
        "    # Perform operations on gradients\n",
        "    for operation in reversed(self.operations):\n",
        "      output_grad = operation.backward(output_grad)\n",
        "\n",
        "    input_grad = output_grad # Store ouput grad after operations\n",
        "\n",
        "    self._param_grads() # Use _param_grads method\n",
        "\n",
        "    return input_grad"
      ],
      "metadata": {
        "id": "BfULFtLvlE4M"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Subclass of a dense layer, i.e. a layer which is full connected\n",
        "class Dense(Layer):\n",
        "  def __init__(self, neurons: int, activation: Operation = Sigmoid()) -> None:\n",
        "    super().__init__(neurons)\n",
        "    self.activation = activation # Initialize the activation function (default: Sigmoid)\n",
        "\n",
        "\n",
        "  def _setup_layer(self, input_: np.ndarray) -> None:\n",
        "    # Use the seed of the Neural Network (if present)\n",
        "    if self.seed:\n",
        "      np.random.seed(self.seed)\n",
        "\n",
        "    self.params = [] # Empty params\n",
        "\n",
        "    self.params.append(np.random.randn(input_.shape[1], self.neurons)) # Initialize weights\n",
        "    self.params.append(np.random.randn(1,self.neurons)) # Initialize bias\n",
        "\n",
        "    self.operations = [WeightMul(self.params[0]), BiasAdd(self.params[1]), self.activation] # Operations of the Dense Layer\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "4eQIQw5O_7j0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss functions"
      ],
      "metadata": {
        "id": "FfhgwUs3KmN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We make a class for the loss function\n",
        "class Loss(object):\n",
        "  def __init__(self) -> None:\n",
        "    pass\n",
        "\n",
        "\n",
        "  def _output(self) -> float: # This method must be defined for each Loss\n",
        "    raise NotImplementedError()\n",
        "\n",
        "\n",
        "  def forward(self, prediction: np.ndarray, target: np.ndarray) -> float:\n",
        "    assert_same_shape(prediction, target) # Check that shapes match\n",
        "\n",
        "    self.prediction = prediction # Store prediction\n",
        "    self.target = target # Store target\n",
        "\n",
        "    loss_value = self._output() # Compute the loss value\n",
        "\n",
        "    return loss_value\n",
        "\n",
        "\n",
        "  def _input_grad(self) -> np.ndarray: # This method must be defined for each Loss\n",
        "    raise NotImplementedError()\n",
        "\n",
        "\n",
        "  def backward(self) -> np.ndarray:\n",
        "    self.input_grad = self._input_grad() # Compute the gradient of the loss wrt the input\n",
        "\n",
        "    assert_same_shape(self.prediction, self.input_grad) # Check that shapes match\n",
        "\n",
        "    return self.input_grad"
      ],
      "metadata": {
        "id": "QjFqXN7pKlM3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Subclass for Mean Squared Error loss function\n",
        "class MeanSquaredError(Loss):\n",
        "  def __init__(self) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "\n",
        "  # Compute MSE per-example\n",
        "  def _output(self) -> float:\n",
        "    loss = np.sum(np.power(self.prediction - self.target, 2)) / self.prediction.shape[0]\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "  # Compute the loss gradient\n",
        "  def _input_grad(self) -> np.ndarray:\n",
        "    return 2.0 * (self.prediction - self.target) / self.prediction.shape[0]"
      ],
      "metadata": {
        "id": "EtTC4vxeNBMT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network"
      ],
      "metadata": {
        "id": "VNzbDzzMhJLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We make a class for Neural Networs\n",
        "class NeuralNetwork(object):\n",
        "  def __init__(self, layers: List[Layer], loss: Loss, seed: float = 42) -> None:\n",
        "    self.layers = layers # Initialize number of layers\n",
        "    self.loss = loss # Initialize loss function\n",
        "    self.seed = seed # Initialize seed (optional, default: 1)\n",
        "    if seed:\n",
        "      for layer in self.layers:\n",
        "        setattr(layer, \"seed\", self.seed)\n",
        "\n",
        "  \n",
        "  # Go forward through layers\n",
        "  def forward(self, x_batch: np.ndarray) -> np.ndarray:\n",
        "    x_out = x_batch\n",
        "    for layer in self.layers:\n",
        "      x_out = layer.forward(x_out)\n",
        "\n",
        "    return x_out\n",
        "\n",
        "\n",
        "  # Go ackward through layers\n",
        "  def backward(self, loss_grad: np.ndarray) -> None:\n",
        "    grad = loss_grad\n",
        "    for layer in reversed(self.layers):\n",
        "      grad = layer.backward(grad)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "  # Go forward through layers, compute loss and go backward through layers\n",
        "  def train_batch(self, x_batch: np.ndarray, y_batch: np.ndarray) -> float:\n",
        "    predictions = self.forward(x_batch)\n",
        "\n",
        "    loss = self.loss.forward(predictions, y_batch)\n",
        "\n",
        "    self.backward(self.loss.backward())\n",
        "\n",
        "    return loss\n",
        "\n",
        "  \n",
        "  # Parameters for the NN\n",
        "  def params(self):\n",
        "    for layer in self.layers:\n",
        "      yield from layer.params\n",
        "    \n",
        "\n",
        "  # Loss gradients wrt parameters for the NN\n",
        "  def param_grads(self):\n",
        "    for layer in self.layers:\n",
        "      yield from layer.param_grads"
      ],
      "metadata": {
        "id": "bUvBi2gIhL_f"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer classes"
      ],
      "metadata": {
        "id": "Iu29zalMntUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We make a class for an optimizer to update paramters of a NN\n",
        "class Optimizer(object):\n",
        "  def __init__(self, learning_rate: float = 0.01):\n",
        "    self.learning_rate = learning_rate # Initialize the learning rate (default: 0.01)\n",
        "\n",
        "\n",
        "  def step(self) -> None:\n",
        "    pass # This method must be defined for each Optimizer"
      ],
      "metadata": {
        "id": "v8WMibO4nSqv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stochastic gradient descent subclass\n",
        "class SGD(Optimizer):\n",
        "  def __init__(self, learning_rate: float = 0.01) -> None:\n",
        "    super().__init__(learning_rate)\n",
        "\n",
        "\n",
        "  def step(self):\n",
        "    for (param, param_grad) in zip(self.net.params(), self.net.param_grads()): #self.net comes from Trainer class\n",
        "      param -= self.learning_rate * param_grad"
      ],
      "metadata": {
        "id": "EL6CTuthopkt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trainer class"
      ],
      "metadata": {
        "id": "38EZ6oe-peNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to shuffle dataset examples\n",
        "def permute_data(X, y):\n",
        "    perm = np.random.permutation(X.shape[0])\n",
        "    return X[perm], y[perm]"
      ],
      "metadata": {
        "id": "XOXU6ktOrtcz"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We make a Trainer class to put together NeuralNetwork and Optimizer\n",
        "class Trainer(object):\n",
        "  def __init__(self, net: NeuralNetwork, optim: Optimizer):\n",
        "    self.net = net # Initialize a neural network\n",
        "    self.optim = optim # Initialize an optimizer\n",
        "    setattr(self.optim, 'net', self.net) # Set net as an attribute of optim\n",
        "\n",
        "    self.best_loss = 1e9\n",
        "\n",
        "\n",
        "  # Generate batches\n",
        "  def generate_batches(self, X: np.ndarray, y: np.ndarray, size: int = 32) -> Tuple[np.ndarray]:\n",
        "\n",
        "        assert X.shape[0] == y.shape[0], \"Features and target must have the same number of rows, instead features has {} and target has {}\".format(X.shape[0], y.shape[0])\n",
        "\n",
        "        N = X.shape[0]\n",
        "\n",
        "        for j in range(0, N, size):\n",
        "            X_batch, y_batch = X[j:j+size], y[j:j+size]\n",
        "\n",
        "            yield X_batch, y_batch\n",
        "  \n",
        "\n",
        "  # Fit the NN on training data for a number of epochs and evaluate it every 'eval_every' epochs\n",
        "  def fit(self, X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray,\n",
        "          epochs: int = 100, eval_every: int = 10, batch_size: int = 32, seed: int = 42, restart: bool = True) -> None:\n",
        "\n",
        "    np.random.seed(seed) # Set the seed\n",
        "\n",
        "    if restart:\n",
        "      for layer in self.net.layers:\n",
        "        layer.first = True\n",
        "\n",
        "      self.best_loss = 1e9\n",
        "\n",
        "    for t in range(epochs):\n",
        "      # Early stopping\n",
        "      if (t+1) % eval_every == 0:\n",
        "        last_model = deepcopy(self.net)\n",
        "\n",
        "      X_train, y_train = permute_data(X_train,y_train) # Shuffle data\n",
        "\n",
        "      batch_generator = self.generate_batches(X_train, y_train, batch_size)\n",
        "\n",
        "      for j, (X_batch, y_batch) in enumerate(batch_generator):\n",
        "        self.net.train_batch(X_batch, y_batch)\n",
        "        self.optim.step()\n",
        "\n",
        "      if (t+1) % eval_every == 0:\n",
        "        test_preds = self.net.forward(X_test)\n",
        "        loss = self.net.loss.forward(test_preds, y_test)\n",
        "        \n",
        "        if loss < self.best_loss:\n",
        "          print(f\"Validation loss after {t+1} epochs is {loss:.3f}.\")\n",
        "          self.best_loss = loss\n",
        "\n",
        "        else:\n",
        "          print(f\"Loss increased after epoch {t+1}, the final loss was {self.best_loss:.3f}, using the model from epoch {t+1-eval_every}\")\n",
        "          self.net = last_model\n",
        "          \n",
        "          setattr(self.optim, 'net', self.net) # Ensure self.optim still update self.net\n",
        "          \n",
        "          break"
      ],
      "metadata": {
        "id": "tVI9lBPUpdpp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part Two: Application to Data"
      ],
      "metadata": {
        "id": "s-vz7Udt4Ozw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some metrics functions\n",
        "\n",
        "# Maen Absolute Error\n",
        "def mae(y_true: np.ndarray, y_pred: np.ndarray):    \n",
        "    return np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "# Root Mean Squared Error\n",
        "def rmse(y_true: np.ndarray, y_pred: np.ndarray):\n",
        "    return np.sqrt(np.mean(np.power(y_true - y_pred, 2)))\n",
        "\n",
        "# Compute MAE and RMSE for a NN\n",
        "def eval_regression_model(model: NeuralNetwork, X_test: np.ndarray, y_test: np.ndarray):\n",
        "    preds = model.forward(X_test)\n",
        "    preds = preds.reshape(-1, 1)\n",
        "    print(\"Mean absolute error: {:.2f}\".format(mae(preds, y_test)))\n",
        "    print(\"\\nRoot mean squared error {:.2f}\".format(rmse(preds, y_test)))"
      ],
      "metadata": {
        "id": "v5NvADGv4UuL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Three Neural Networks"
      ],
      "metadata": {
        "id": "tSxsIXWO45-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 layer nn (aka linear regression)\n",
        "linear_regression = NeuralNetwork(\n",
        "    layers = [Dense(neurons = 1, activation = Linear())],\n",
        "    loss = MeanSquaredError(),\n",
        "    seed = 42\n",
        ")\n",
        "\n",
        "# 2 layer nn\n",
        "nn = NeuralNetwork(\n",
        "    layers = [Dense(neurons = 13, activation = Sigmoid()),\n",
        "              Dense(neurons = 1, activation = Linear())],\n",
        "    loss = MeanSquaredError(),\n",
        "    seed = 42\n",
        ")\n",
        "\n",
        "# 3 layer nn\n",
        "deep_nn = NeuralNetwork(\n",
        "    layers = [Dense(neurons = 13, activation = Sigmoid()),\n",
        "              Dense(13, activation = Sigmoid()),\n",
        "              Dense(1, activation = Linear())],\n",
        "    loss = MeanSquaredError(),\n",
        "    seed = 42\n",
        ")"
      ],
      "metadata": {
        "id": "C5cb1vUP45AX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Data"
      ],
      "metadata": {
        "id": "JTfFFDQ67qkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "\n",
        "boston = load_boston()\n",
        "data = boston.data\n",
        "target = boston.target\n",
        "features = boston.feature_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCLesMtS7tk_",
        "outputId": "75c0d197-fb78-431c-e692-0ecec5aa095e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "data = scaler.fit_transform(data)"
      ],
      "metadata": {
        "id": "GyvYUfLT7vzG"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to turn 1D arrays into 2D arrays \n",
        "def to_2d_np(a: np.ndarray, type: str = \"col\") -> np.ndarray:\n",
        "\n",
        "    assert a.ndim == 1, \"Input tensors must be 1 dimensional\"\n",
        "    \n",
        "    if type == \"col\":        \n",
        "        return a.reshape(-1, 1)\n",
        "    elif type == \"row\":\n",
        "        return a.reshape(1, -1)"
      ],
      "metadata": {
        "id": "lqCy3U9j77Fv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split train and test and make targets 2D arrays\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=80718)\n",
        "\n",
        "y_train, y_test = to_2d_np(y_train), to_2d_np(y_test)"
      ],
      "metadata": {
        "id": "EU-mpWFY8J3d"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the three NNs"
      ],
      "metadata": {
        "id": "NM69ZycH8WU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train linear regression\n",
        "trainer = Trainer(linear_regression, SGD(learning_rate = 0.01))\n",
        "\n",
        "trainer.fit(X_train, y_train, X_test, y_test, epochs = 50, eval_every = 10, seed = 42);\n",
        "print()\n",
        "eval_regression_model(linear_regression, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiS76aUv8V0-",
        "outputId": "e2b27398-f079-4793-e0a1-7cf19f2187e0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss after 10 epochs is 37.226.\n",
            "Validation loss after 20 epochs is 29.312.\n",
            "Validation loss after 30 epochs is 26.731.\n",
            "Validation loss after 40 epochs is 26.571.\n",
            "Loss increased after epoch 50, the final loss was 26.571, using the model from epoch 40\n",
            "\n",
            "Mean absolute error: 3.68\n",
            "\n",
            "Root mean squared error 5.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train 2 layers nn\n",
        "trainer = Trainer(nn, SGD(learning_rate = 0.01))\n",
        "\n",
        "trainer.fit(X_train, y_train, X_test, y_test, epochs = 50, eval_every = 10, seed = 42);\n",
        "print()\n",
        "eval_regression_model(nn, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TKenSAm-oif",
        "outputId": "d2200ba1-c3d8-412b-c817-0d847e2ac7c7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss after 10 epochs is 35.272.\n",
            "Validation loss after 20 epochs is 20.043.\n",
            "Validation loss after 30 epochs is 17.688.\n",
            "Validation loss after 40 epochs is 15.706.\n",
            "Validation loss after 50 epochs is 14.666.\n",
            "\n",
            "Mean absolute error: 2.53\n",
            "\n",
            "Root mean squared error 3.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train 3 layers nn\n",
        "trainer = Trainer(deep_nn, SGD(learning_rate = 0.01))\n",
        "\n",
        "trainer.fit(X_train, y_train, X_test, y_test, epochs = 50, eval_every = 10, seed = 42);\n",
        "print()\n",
        "eval_regression_model(deep_nn, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clwXaiCZAYdq",
        "outputId": "1588107b-857d-428f-94a7-8e2e3e3be471"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss after 10 epochs is 89.403.\n",
            "Validation loss after 20 epochs is 19.193.\n",
            "Validation loss after 30 epochs is 16.681.\n",
            "Validation loss after 40 epochs is 14.424.\n",
            "Validation loss after 50 epochs is 12.989.\n",
            "\n",
            "Mean absolute error: 2.37\n",
            "\n",
            "Root mean squared error 3.60\n"
          ]
        }
      ]
    }
  ]
}